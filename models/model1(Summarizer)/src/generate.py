# -*- coding: utf-8 -*-
"""generate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pE-B7eW7jpR8X9kKKEAwdoC7UvrhmGmW
"""

import torch
from transformers import LEDForConditionalGeneration, AutoTokenizer
from config import OUTPUT_DIR, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = LEDForConditionalGeneration.from_pretrained(OUTPUT_DIR).to(device)
tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)

def generate_summary(text):

    inputs = tokenizer(
        text,
        max_length=MAX_INPUT_LENGTH,
        truncation=True,
        padding="max_length",
        return_tensors="pt"
    ).to(device)

    global_attention_mask = torch.zeros_like(inputs["input_ids"])
    global_attention_mask[:, 0] = 1

    summary_ids = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        global_attention_mask=global_attention_mask,
        num_beams=4,
        max_length=MAX_TARGET_LENGTH,
        early_stopping=True
    )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

if __name__ == "__main__":
    text = input("Enter text to summarize:\n\n")
    print("\nSUMMARY:\n", generate_summary(text))