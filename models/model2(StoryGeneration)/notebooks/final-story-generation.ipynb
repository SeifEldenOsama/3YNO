{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yPuBI6HC-IkF",
   "metadata": {
    "id": "yPuBI6HC-IkF"
   },
   "source": [
    "## Install Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v_1YTMAt8Kfo",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyyaml roman python-Levenshtein transformers scipy langchain langchain-core langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DvA-HhL0aKKe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeWS_wIqaVv4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D7kc3rSF-Q5l",
   "metadata": {
    "id": "D7kc3rSF-Q5l"
   },
   "source": [
    "## preparing LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WmBve5eo7iFE",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial imports and logging configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import yaml\n",
    "import roman\n",
    "import Levenshtein\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.special import log_softmax\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)-8s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "print(\"Initial imports and logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Vacdlwd77iFF",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCALHOST = 'http://localhost'\n",
    "DEFAULT_PORT = 8000\n",
    "\n",
    "class ServerConfig:\n",
    "    def __init__(self, engine, host, port, server_type, tensor_parallel_size):\n",
    "        self.engine = engine\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.server_type = server_type\n",
    "        self.tensor_parallel_size = tensor_parallel_size\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        return ServerConfig(\n",
    "            engine=config['engine'],\n",
    "            host=config['host'],\n",
    "            port=config.get('port', DEFAULT_PORT),\n",
    "            server_type=config['server_type'],\n",
    "            tensor_parallel_size=config['tensor_parallel_size']\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.engine, self.host, self.port,\n",
    "                     self.server_type, self.tensor_parallel_size))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return (self.engine, self.host, self.port, self.server_type, self.tensor_parallel_size) == (other.engine, other.host, other.port, other.server_type, self.tensor_parallel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oIYF4BvC7iFF",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_lowercase_keys(d):\n",
    "    if type(d) is dict:\n",
    "        new_d = {}\n",
    "        for key in d:\n",
    "            new_d[key.lower()] = recursive_lowercase_keys(d[key])\n",
    "        return new_d\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, config, parent=None):\n",
    "        self.parent_config = parent\n",
    "        self.config = config\n",
    "        for key in self.config:\n",
    "            if type(self.config[key]) is dict:\n",
    "                self.config[key] = Config(self.config[key], self)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_dict(all_confs, config_names):\n",
    "        config = {}\n",
    "        for name in config_names:\n",
    "            if ',' in name:\n",
    "                for n in name.split(','):\n",
    "                    config.update(all_confs[n])\n",
    "            else:\n",
    "                config.update(all_confs[name])\n",
    "\n",
    "        return Config(config, None)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.config:\n",
    "            return self.config[name]\n",
    "        elif self.parent_config is not None:\n",
    "            return getattr(self.parent_config, name)\n",
    "        else:\n",
    "            raise AttributeError(f\"Config has no attribute {name}.\")\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def __contains__(self, name):\n",
    "        return name in self.config\n",
    "\n",
    "    def get(self, name, default=None):\n",
    "        try:\n",
    "            return self[name]\n",
    "        except AttributeError:\n",
    "            return default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_mkwMYxd7iFF",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = {}\n",
    "\n",
    "def init_logging(logging_level):\n",
    "    logging_level = logging_level.upper()\n",
    "    assert logging_level in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']\n",
    "    logging.getLogger().setLevel(logging_level)\n",
    "\n",
    "class TimeoutException(Exception): pass\n",
    "\n",
    "@contextmanager\n",
    "def time_limit(seconds):\n",
    "    def signal_handler(signum, frame):\n",
    "        raise TimeoutException(\"Timed out!\")\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "class Filter:\n",
    "    def __init__(self, filter_func):\n",
    "        self.filter_func = filter_func\n",
    "\n",
    "    @staticmethod\n",
    "    def wrap_preprocessor(preprocessor, filter):\n",
    "        return Filter(lambda s: filter(preprocessor(s)))\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        try:\n",
    "            return self.filter_func(*args, **kwargs)\n",
    "        except:\n",
    "            return self.filter_func(*args)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Filter(lambda s: self.filter_func(s) and other.filter_func(s))\n",
    "\n",
    "def min_max_tokens_filter(min_tokens, max_tokens, tokenizer_model_string='gpt2', filter_empty=True):\n",
    "    global tokenizers\n",
    "    if tokenizer_model_string not in tokenizers:\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_string)\n",
    "            tokenizers[tokenizer_model_string] = tokenizer\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not load tokenizer {tokenizer_model_string}. Token filtering will be skipped. Error: {e}\")\n",
    "            return Filter(lambda s: True)\n",
    "    else:\n",
    "        tokenizer = tokenizers[tokenizer_model_string]\n",
    "\n",
    "    filter_func = Filter(lambda s: min_tokens <= len(tokenizer.encode(s.strip())) <= max_tokens)\n",
    "    if filter_empty:\n",
    "        filter_func = filter_func + Filter(lambda s: len(s.strip()) > 0)\n",
    "    return filter_func\n",
    "\n",
    "def levenshtein_ratio_filter(passages_to_match, threshold=0.8):\n",
    "    return Filter(lambda s: all([all([Levenshtein.ratio(sub_s, passage) < threshold for passage in passages_to_match]) for sub_s in s.split()]))\n",
    "\n",
    "def word_filter(word_list):\n",
    "    return Filter(lambda s: all([word not in s for word in word_list]))\n",
    "\n",
    "def list_next_number_format_filter():\n",
    "    bad_regex = re.compile(r'[^]\\d+\\.')\n",
    "    return Filter(lambda s: not bad_regex.search(s))\n",
    "\n",
    "def extract_choice_logprobs(full_completion, choices=['yes', 'no'], default_logprobs=[-1e8, -1e8], case_sensitive=False):\n",
    "    batch_logprobs = []\n",
    "    for choice in full_completion['choices']:\n",
    "        all_logprobs = choice['logprobs']['top_logprobs']\n",
    "        found = False\n",
    "        logprobs = [l for l in default_logprobs]\n",
    "        for token_logprobs in all_logprobs:\n",
    "            for key, value in token_logprobs.items():\n",
    "                for i, choice in enumerate(choices):\n",
    "                    if choice in key or (not case_sensitive and choice.lower() in key.lower()):\n",
    "                        found = True\n",
    "                        logprobs[i] = value\n",
    "            if found:\n",
    "                break\n",
    "        batch_logprobs.append(log_softmax(logprobs))\n",
    "    return batch_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cP8A8MDk7iFG",
   "metadata": {},
   "outputs": [],
   "source": [
    "warned_prompt_format = {'openai_response_prefix': False}\n",
    "\n",
    "def format_langchain_prompt(langchain_prompt, **kwargs):\n",
    "    kwargs = {k: v for k, v in kwargs.items() if k in langchain_prompt.input_variables}\n",
    "    return langchain_prompt.format(**kwargs)\n",
    "\n",
    "class TemplatePromptBuilder:\n",
    "    def __init__(self, base_dict):\n",
    "        self.instruction = PromptTemplate.from_template(template=base_dict['instruction'],)\n",
    "        self.system_message = PromptTemplate.from_template(template=base_dict['system_message'],) if 'system_message' in base_dict else None\n",
    "        self.response_prefix = PromptTemplate.from_template(template=base_dict['response_prefix'],) if 'response_prefix' in base_dict else None\n",
    "        self.output_prefix = PromptTemplate.from_template(template=base_dict['output_prefix'],) if 'output_prefix' in base_dict else None\n",
    "\n",
    "    def format(self, **kwargs):\n",
    "        return PromptBuilder(self, **kwargs)\n",
    "\n",
    "class PromptBuilder:\n",
    "    def __init__(self, template_prompt_builder, **kwargs):\n",
    "        self.instruction = format_langchain_prompt(template_prompt_builder.instruction, **kwargs)\n",
    "        self.system_message = format_langchain_prompt(template_prompt_builder.system_message, **kwargs) \\\n",
    "            if template_prompt_builder.system_message is not None else None\n",
    "        self.response_prefix = format_langchain_prompt(template_prompt_builder.response_prefix, **kwargs) \\\n",
    "            if template_prompt_builder.response_prefix is not None else None\n",
    "        self.output_prefix = format_langchain_prompt(template_prompt_builder.output_prefix, **kwargs) \\\n",
    "            if template_prompt_builder.output_prefix is not None else None\n",
    "\n",
    "    def render_for_llm_format(self, prompt_format):\n",
    "        if prompt_format not in ['openai-chat', 'llama2-chat', 'none']:\n",
    "            raise NotImplementedError(f\"Prompt format {prompt_format} not implemented.\")\n",
    "\n",
    "        prompt = self.instruction.format().lstrip()\n",
    "\n",
    "        if prompt_format == 'openai-chat':\n",
    "            if self.response_prefix is not None:\n",
    "                global warned_prompt_format\n",
    "                if warned_prompt_format['openai_response_prefix']:\n",
    "                    logging.warning(f\"Response prefix is not supported for prompt format {prompt_format}. Appending to end of instruction instead.\")\n",
    "                    warned_prompt_format['openai_response_prefix'] = True\n",
    "                prompt += '\\n\\n\\n\\nThe output is already partially generated. Continue from:\\n\\n' + self.response_prefix.format()\n",
    "            messages = [{'role': 'user', 'content': prompt}]\n",
    "            if self.system_message is not None:\n",
    "                messages = [{'role': 'system', 'content': self.system_message.format()}] + messages\n",
    "            return messages\n",
    "\n",
    "        else:\n",
    "            if prompt_format == 'llama2-chat':\n",
    "                prompt = '[INST]'\n",
    "                if self.system_message is not None:\n",
    "                    prompt += ' <<SYS>>\\n' + self.system_message.format() + '\\n<</SYS>>\\n\\n'\n",
    "                else:\n",
    "                    prompt += ' '\n",
    "                prompt += self.instruction.format()\n",
    "                prompt += '[/INST]' if self.instruction.format()[-1] == ' ' else ' [/INST]'\n",
    "                if self.response_prefix is not None:\n",
    "                    prompt += self.response_prefix.format() if self.response_prefix.format()[0] == ' ' else ' ' + self.response_prefix.format()\n",
    "            else:\n",
    "                if self.system_message is not None:\n",
    "                    prompt = self.system_message.format() + '\\n\\n\\n\\n' + prompt\n",
    "                if self.response_prefix is not None:\n",
    "                    prompt = prompt + '\\n\\n\\n\\n' + self.response_prefix.format()\n",
    "            return prompt\n",
    "\n",
    "def _create_prompt_templates(prompts):\n",
    "    for key in prompts:\n",
    "        assert isinstance(prompts[key], dict)\n",
    "        if 'instruction' not in prompts[key]:\n",
    "            _create_prompt_templates(prompts[key])\n",
    "        else:\n",
    "            prompts[key] = TemplatePromptBuilder(prompts[key])\n",
    "\n",
    "def load_prompts_from_dict(prompts_dict):\n",
    "    prompts = prompts_dict.copy()\n",
    "    _create_prompt_templates(prompts)\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Bdjq7ZP67iFG",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 14:37:32.207416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764945452.365042     242 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764945452.411622     242 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 14:37:42 INFO     NumExpr defaulting to 4 threads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4f156c061949bb8ff086747a7df263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import openai\n",
    "models = {}\n",
    "\n",
    "class SamplingConfig:\n",
    "    def __init__(self,\n",
    "                 server_config,\n",
    "                 prompt_format,\n",
    "                 max_tokens=None,\n",
    "                 temperature=None,\n",
    "                 top_p=None,\n",
    "                 frequency_penalty=None,\n",
    "                 presence_penalty=None,\n",
    "                 stop=None,\n",
    "                 n=None,\n",
    "                 logit_bias=None,\n",
    "                 logprobs=None):\n",
    "        self.server_config = server_config\n",
    "        self.prompt_format = prompt_format\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.frequency_penalty = frequency_penalty\n",
    "        self.presence_penalty = presence_penalty\n",
    "        self.stop = stop\n",
    "        self.n = n\n",
    "        self.logit_bias = logit_bias\n",
    "        self.logprobs = logprobs\n",
    "\n",
    "    @staticmethod\n",
    "    def from_config(config):\n",
    "        return SamplingConfig(\n",
    "            server_config=ServerConfig.from_config(config),\n",
    "            prompt_format=config['prompt_format'],\n",
    "            max_tokens=config.get('max_tokens', None),\n",
    "            temperature=config.get('temperature', None),\n",
    "            top_p=config.get('top_p', None),\n",
    "            frequency_penalty=config.get('frequency_penalty', None),\n",
    "            presence_penalty=config.get('presence_penalty', None),\n",
    "            stop=config.get('stop', None),\n",
    "            n=config.get('n', None),\n",
    "            logit_bias=config.get('logit_bias', None),\n",
    "            logprobs=config.get('logprobs', None)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "    def dict(self):\n",
    "        d = {'model': self.server_config.engine}\n",
    "        for attr in ['max_tokens', 'temperature', 'top_p', 'frequency_penalty', 'presence_penalty', 'stop', 'n', 'logit_bias', 'logprobs']:\n",
    "            if getattr(self, attr) is not None:\n",
    "                d[attr] = getattr(self, attr)\n",
    "        return d\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "class LLMClient:\n",
    "\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def call_with_retry(self, prompt_builder, sampling_config, postprocessor=None,\n",
    "                        filter=lambda s: True, max_attempts=5, **kwargs):\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                completions, full_obj = self(prompt_builder, sampling_config, **kwargs)\n",
    "\n",
    "                if postprocessor:\n",
    "                    completions = postprocessor(completions)\n",
    "\n",
    "                completions = [c for c in completions if filter(c)]\n",
    "                if completions:\n",
    "                    return completions, full_obj\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR attempt {attempt+1}: {e}\")\n",
    "\n",
    "        raise RuntimeError(\"Failed after retries.\")\n",
    "\n",
    "    def __call__(self, prompt_builder, sampling_config, **kwargs):\n",
    "        messages = prompt_builder.render_for_llm_format(sampling_config.prompt_format)\n",
    "\n",
    "        prompt = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "        max_tokens = sampling_config.max_tokens if sampling_config.max_tokens is not None else 512\n",
    "        temperature = sampling_config.temperature if sampling_config.temperature is not None else 1.0\n",
    "        top_p = sampling_config.top_p if sampling_config.top_p is not None else 1.0\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        generated_ids = outputs[0, inputs.input_ids.shape[-1]:]\n",
    "        text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "        return [text], None\n",
    "\n",
    "llm_client = LLMClient(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KSNEplYT-Ykl",
   "metadata": {
    "id": "KSNEplYT-Ykl"
   },
   "source": [
    "## Extract Premise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "KRrgf_vL7iFH",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Premise:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return Premise(data['title'], data['premise'])\n",
    "\n",
    "    def __init__(self, title=None, premise=None):\n",
    "        self.title = title\n",
    "        self.premise = premise\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Title: {self.title}\\n\\nPremise: {self.premise}'\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'title': self.title,\n",
    "                'premise': self.premise\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "I4GYK-mi7iFH",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "config_yaml_content = \"\"\"\n",
    "defaults:\n",
    "  output_path: output/premise.json\n",
    "  logging_level: info\n",
    "  MODEL:\n",
    "    engine: mistralai/Mistral-7B-Instruct-v0.3\n",
    "    tensor_parallel_size: 1\n",
    "    server_type: vllm\n",
    "    host: http://localhost\n",
    "    port: 9741\n",
    "    prompt_format: openai-chat\n",
    "    temperature: 0.7\n",
    "    top_p: 0.95\n",
    "    frequency_penalty: 0\n",
    "    presence_penalty: 0\n",
    "    TITLE:\n",
    "      max_tokens: 32\n",
    "      stop: []\n",
    "    PREMISE:\n",
    "      max_tokens: 200\n",
    "      stop: [\"\\n\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "all_confs = recursive_lowercase_keys(yaml.safe_load(config_yaml_content ))\n",
    "config = Config.load_from_dict(all_confs, ['defaults'])\n",
    "\n",
    "print(\"Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "xNrfe6Pf7iFH",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title(premise_object, title_prompts, title_config, llm_client):\n",
    "    title = llm_client.call_with_retry(\n",
    "        title_prompts.format(educational_summary_input=educational_summary_input),\n",
    "        SamplingConfig.from_config(title_config),\n",
    "        filter=min_max_tokens_filter(0, title_config['max_tokens'])\n",
    "    )[0]\n",
    "    premise_object.title = title\n",
    "    return premise_object\n",
    "\n",
    "\n",
    "def generate_premise(premise_object, premise_prompts, premise_config, llm_client):\n",
    "    premise = llm_client.call_with_retry(\n",
    "        premise_prompts.format(\n",
    "            title=premise_object.title,\n",
    "            educational_summary_input=educational_summary_input\n",
    "        ),\n",
    "        SamplingConfig.from_config(premise_config),\n",
    "        filter=min_max_tokens_filter(0, premise_config['max_tokens'])\n",
    "    )[0]\n",
    "    premise_object.premise = premise\n",
    "    return premise_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "xzDJzSqa7iFH",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts loaded and templates created.\n"
     ]
    }
   ],
   "source": [
    "educational_summary_input = \"\"\"\n",
    "Plants are living things that need care to grow strong and healthy. Every plant starts as a tiny seed. When the seed is placed in soil and given water, it begins to wake up. Soon, small roots grow down into the soil to drink water and collect minerals. After that, a little stem grows upward, reaching for the sunlight.\n",
    "\n",
    "Plants use sunlight to make their own food in a process called photosynthesis. This helps them grow leaves, flowers, and sometimes fruits or vegetables. Different plants need different amounts of water and sunlight, but all of them need love, attention, and patience. By taking care of plants, children learn responsibility and understand how nature works around them.\n",
    "\"\"\"\n",
    "\n",
    "prompts_json_content = \"\"\"\n",
    "{\n",
    "   \"title\": {\n",
    "     \"instruction\": \"Write a fun, simple, and playful title for a children's story based on this summary: {educational_summary_input}. Keep it short and exciting.\",\n",
    "     \"response_prefix\": \"\"\n",
    "   },\n",
    "   \"premise\": {\n",
    "     \"instruction\": \"Write a one-paragraph story premise suitable for kids. Describe the world, the main character, and the adventure. Use simple words, short sentences, and fun imagery. Educational summary: {educational_summary_input}. Do NOT include the word 'Title' or any headings.\",\n",
    "     \"response_prefix\": \"\"\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prompts_dict = json.loads(prompts_json_content)\n",
    "prompts = load_prompts_from_dict(prompts_dict)\n",
    "print(\"Prompts loaded and templates created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2B67Dcu97iFI",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 15:11:13 INFO     Starting premise generation...\n",
      "2025-12-05 15:11:13 INFO     Generating title...\n",
      "2025-12-05 15:11:16 INFO     Generated title: ['\"Seed Sprouts Surprise: A Sunlit Tale of Growth and Green Thumbs\"']\n",
      "2025-12-05 15:11:16 INFO     Generating premise...\n",
      "2025-12-05 15:11:41 INFO     Generated premise: [\"In a vibrant, sun-kissed Gardenia Village, where every house has a blooming garden, lives a curious and kind-hearted child named Marigold. One sunny morning, Marigold finds a small, sleepy seed in her grandmother's garden. With a twinkle in her eyes, she decides to nurture this tiny promise of a plant. Marigold carefully places the seed in a pot filled with nutrient-rich soil and waters it gently. As days pass, she watches and waits, watering and talking to her new friend, hoping and praying for it to grow. One magical day, a green shoot pokes through the soil, reaching up towards the sky. Marigold cheers, realizing that she's not just growing a plant, but also learning the secrets of life - patience, love, and the magic of nature. This exciting adventure of growing a plant from a seed teaches Marigold about responsibility\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL RESULT ---\n",
      "Title: ['\"Seed Sprouts Surprise: A Sunlit Tale of Growth and Green Thumbs\"']\n",
      "\n",
      "Premise: [\"In a vibrant, sun-kissed Gardenia Village, where every house has a blooming garden, lives a curious and kind-hearted child named Marigold. One sunny morning, Marigold finds a small, sleepy seed in her grandmother's garden. With a twinkle in her eyes, she decides to nurture this tiny promise of a plant. Marigold carefully places the seed in a pot filled with nutrient-rich soil and waters it gently. As days pass, she watches and waits, watering and talking to her new friend, hoping and praying for it to grow. One magical day, a green shoot pokes through the soil, reaching up towards the sky. Marigold cheers, realizing that she's not just growing a plant, but also learning the secrets of life - patience, love, and the magic of nature. This exciting adventure of growing a plant from a seed teaches Marigold about responsibility\"]\n",
      "\n",
      "Premise object saved to: output/premise.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    init_logging(config.logging_level)\n",
    "    logging.info(\"Starting premise generation...\")\n",
    "\n",
    "\n",
    "    premise = Premise()\n",
    "\n",
    "    logging.info(\"Generating title...\")\n",
    "    generate_title(premise, prompts['title'], config['model']['title'], llm_client)\n",
    "    logging.info(f'Generated title: {premise.title}')\n",
    "\n",
    "    logging.info(\"Generating premise...\")\n",
    "\n",
    "    generate_premise(premise, prompts['premise'], config['model']['premise'], llm_client)\n",
    "    logging.info(f'Generated premise: {premise.premise}')\n",
    "\n",
    "    output_path = config['output_path']\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    premise.save(output_path)\n",
    "\n",
    "    print(\"\\n--- FINAL RESULT ---\")\n",
    "    print(premise)\n",
    "    print(f\"\\nPremise object saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during execution. Please check your model configuration and ensure your LLM server is running and accessible. Error: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yjVzYhRP-gtG",
   "metadata": {
    "id": "yjVzYhRP-gtG"
   },
   "source": [
    "## Extract Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dJB-YVL6bXvD",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "config_yaml_content = \"\"\"\n",
    "defaults:\n",
    "  premise_path: output/premise.json\n",
    "  output_path: output/plan.json\n",
    "  logging_level: info\n",
    "  MODEL:\n",
    "    engine: mistralai/Mistral-7B-Instruct-v0.3\n",
    "    tensor_parallel_size: 1\n",
    "    server_type: vllm\n",
    "    host: http://localhost\n",
    "    port: 9741\n",
    "    prompt_format: openai-chat\n",
    "    temperature: 0.7\n",
    "    top_p: 0.9\n",
    "    frequency_penalty: 0\n",
    "    presence_penalty: 0\n",
    "    PLAN:\n",
    "      SETTING:\n",
    "        max_tokens: 256\n",
    "        stop: []\n",
    "      ENTITY:\n",
    "        max_attempts: 2\n",
    "        min_entities: 1\n",
    "        max_entities: 4\n",
    "        NAME:\n",
    "          max_tokens: 16\n",
    "          stop: [\"\\n\", \",\", \":\", \"(\"]\n",
    "        DESCRIPTION:\n",
    "          max_tokens: 60\n",
    "      OUTLINE:\n",
    "        max_attempts: 2\n",
    "        expansion_policy: breadth-first\n",
    "        max_depth: 1\n",
    "        context: ancestors-with-siblings-children\n",
    "        min_children: 1\n",
    "        preferred_max_children: 2\n",
    "        max_children: 3\n",
    "        EVENT_DEPTH_0:\n",
    "          max_tokens: 100\n",
    "        EVENT:\n",
    "          frequency_penalty: 0.3\n",
    "          max_tokens: 100\n",
    "        SCENE:\n",
    "          context: ancestors-with-siblings\n",
    "          max_tokens: 80\n",
    "        ENTITY_DEPTH_0:\n",
    "          max_tokens: 80\n",
    "        ENTITY:\n",
    "          max_tokens: 80\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "all_confs = recursive_lowercase_keys(yaml.safe_load(config_yaml_content ))\n",
    "config = Config.load_from_dict(all_confs, ['defaults'])\n",
    "print(\"Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "v5xQvshCFE9_",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts loaded and templates created.\n"
     ]
    }
   ],
   "source": [
    "prompts_json_content = \"\"\"\n",
    "{\n",
    "  \"plan\": {\n",
    "    \"setting\": {\n",
    "      \"instruction\": \"Create a fun, colorful, and simple setting for a children's story. Use short sentences and words kids understand. Show what it looks like, sounds like, and feels like. Title: {title}//Premise: {premise}\",\n",
    "      \"response_prefix\": \"\"\n",
    "    },\n",
    "    \"entity\": {\n",
    "      \"name\": {\n",
    "        \"instruction\": \"Generate only the next character's name for a children's story. Use fun and simple names. Output only the name. Do not repeat previous names. Title: {title}//Premise: {premise}//Setting: {setting}//Existing Characters: {entity_list}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      },\n",
    "      \"description\": {\n",
    "        \"instruction\": \"Describe the character in one simple sentence. Include what they look like and what makes them special. Avoid hard words. Title: {title}//Premise: {premise}//Setting: {setting}//Character: {entity_name}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      }\n",
    "    },\n",
    "    \"outline\": {\n",
    "      \"event_depth_0\": {\n",
    "        \"instruction\": \"Write the first important event of the story in one short, clear sentence for kids. Only describe the event. Title: {title}//Premise: {premise}//Setting: {setting}//Characters: {entities}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      },\n",
    "      \"entity_depth_0\": {\n",
    "        \"instruction\": \"List all main characters appearing in this top-level event. Output them as a comma-separated list. Only use names from the global entity list. Do not invent new names. Title: {title}//Premise: {premise}//Setting: {setting}//Event: {current_event}//Detected Entities: {detected_entities}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      },\n",
    "      \"event\": {\n",
    "        \"instruction\": \"Write the next event in the story as one simple sentence. Keep it fun, clear, and easy to understand for kids. Title: {title}//Premise: {premise}//Setting: {setting}//Characters: {entities}//Outline so far://{context_prefix}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      },\n",
    "      \"scene\": {\n",
    "        \"instruction\": \"Describe where this event happens in one sentence. Keep it easy to picture and kid-friendly. Title: {title}//Premise: {premise}//Setting: {setting}//Characters: {entities}//Event: {current_event}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      },\n",
    "      \"entity\": {\n",
    "        \"instruction\": \"Identify all characters present in this event. Use only names from the main entity list. Return them as a comma-separated list. Title: {title}//Premise: {premise}//Setting: {setting}//Event: {current_event}//Scene: {current_scene}//Detected Entities: {detected_entities}\",\n",
    "        \"response_prefix\": \"\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "prompts_dict = json.loads(prompts_json_content)\n",
    "prompts = load_prompts_from_dict(prompts_dict)\n",
    "print(\"Prompts loaded and templates created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "-QEO-PY1-GAI",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import string\n",
    "from collections.abc import Sequence\n",
    "from functools import partial\n",
    "import string\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7YQDaJnWFiq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Setting:\n",
    "    def __init__(self, setting):\n",
    "        if isinstance(setting, list) and setting:\n",
    "            self.setting = setting[0]\n",
    "        else:\n",
    "            self.setting = setting\n",
    "\n",
    "    def __str__(self):\n",
    "        if isinstance(self.setting, str):\n",
    "            return self.setting\n",
    "        return str(self.setting)\n",
    "\n",
    "class Plan:\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        premise = Premise(data['premise']['title'], data['premise']['premise'])\n",
    "        setting = Setting(data['setting'])\n",
    "\n",
    "        flat = []\n",
    "\n",
    "        def add_item(obj):\n",
    "            if obj is None:\n",
    "                return\n",
    "            if isinstance(obj, Entity):\n",
    "                flat.append(obj)\n",
    "            elif isinstance(obj, dict):\n",
    "                if \"name\" in obj and \"description\" in obj:\n",
    "                    flat.append(Entity(obj[\"name\"], obj[\"description\"]))\n",
    "            elif isinstance(obj, list):\n",
    "                for sub in obj:\n",
    "                    add_item(sub)\n",
    "\n",
    "        add_item(data[\"entities\"])\n",
    "\n",
    "        entity_list = EntityList(flat)\n",
    "\n",
    "        outline = OutlineNode.from_dict(data['outline'])\n",
    "\n",
    "        return Plan(premise, setting, entity_list, outline)\n",
    "\n",
    "\n",
    "    def __init__(self, premise, setting=None, entity_list=None, outline=None):\n",
    "        self.premise = premise\n",
    "        self.setting = setting\n",
    "        self.entity_list = entity_list\n",
    "        self.outline = outline\n",
    "\n",
    "    def __str__(self):\n",
    "        premise_str = str(self.premise) if self.premise is not None else \"\"\n",
    "\n",
    "        def flatten_list(x):\n",
    "            if isinstance(x, list):\n",
    "                flat = []\n",
    "                for item in x:\n",
    "                    if isinstance(item, list):\n",
    "                        flat.extend(flatten_list(item))\n",
    "                    else:\n",
    "                        flat.append(item)\n",
    "                return flat\n",
    "            return [x]\n",
    "\n",
    "        try:\n",
    "            setting_items = flatten_list(self.setting)\n",
    "            setting_str = \"\\n\".join(str(s) for s in setting_items)\n",
    "        except:\n",
    "            setting_str = str(self.setting.setting) if self.setting and hasattr(self.setting, 'setting') else str(self.setting)\n",
    "\n",
    "        try:\n",
    "            if hasattr(self.entity_list, 'entities'):\n",
    "                entities_str = str(self.entity_list)\n",
    "            else:\n",
    "                entities_str = \"\\n\".join(\n",
    "                    f\"{i+1}. {str(e.name)}: {str(e.description)}\"\n",
    "                    for i, e in enumerate(self.entity_list)\n",
    "                )\n",
    "        except Exception as e:\n",
    "                entities_str = str(self.entity_list)\n",
    "                logging.error(f\"Error formatting entities: {e}\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            outline_str = str(self.outline)\n",
    "        except:\n",
    "            outline_str = str(self.outline)\n",
    "\n",
    "\n",
    "        return (\n",
    "            f\"{premise_str}\\n\\n\"\n",
    "            f\"Setting:\\n{setting_str}\\n\\n\"\n",
    "            f\"Characters and Entities:\\n{entities_str}\\n\\n\"\n",
    "            f\"Outline:\\n{outline_str}\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump({\n",
    "                'premise': {\n",
    "                    'title': self.premise.title,\n",
    "                    'premise': self.premise.premise\n",
    "                },\n",
    "                'setting': self.setting.setting,\n",
    "                'entities': [{\n",
    "                    'name': entity.name,\n",
    "                    'description': entity.description\n",
    "                } for entity in self.entity_list],\n",
    "                'outline': self.outline.to_dict()\n",
    "            }, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "jhvFApbAFjVY",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "    _ = stopwords.words('english')\n",
    "except:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, name, description):\n",
    "\n",
    "        if isinstance(name, list):\n",
    "            name = name[0] if name else \"\"\n",
    "        self.name = str(name).strip()\n",
    "\n",
    "        if isinstance(description, list):\n",
    "            description = description[0] if description else \"\"\n",
    "        self.description = str(description).strip()\n",
    "\n",
    "\n",
    "\n",
    "class EntityList:\n",
    "    def __init__(self, entities=None):\n",
    "        self.entities = entities if entities is not None else []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.entities)\n",
    "\n",
    "    def __str__(self):\n",
    "        lines = []\n",
    "        for i, entity in enumerate(self.entities):\n",
    "            name = entity.name\n",
    "            desc = entity.description\n",
    "\n",
    "            if isinstance(name, list):\n",
    "                name = name[0] if name else \"\"\n",
    "            if isinstance(desc, list):\n",
    "                desc = desc[0] if desc else \"\"\n",
    "\n",
    "            lines.append(f\"{i+1}. {str(name)}: {str(desc)}\")\n",
    "\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "    def print_with_full_names(self):\n",
    "        return '\\n\\n'.join([f'{i+1}. Full Name: {entity.name}\\n\\nDescription: {entity.description}' for i, entity in enumerate(self.entities)])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.entities)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.entities[index]\n",
    "\n",
    "    def get_entity_by_name(self, name):\n",
    "        for entity in self.entities:\n",
    "            if entity.name == name:\n",
    "                return entity\n",
    "        raise ValueError(f'EntityList has no entity named {name}.')\n",
    "\n",
    "\n",
    "def detect_entities(event, entity_list):\n",
    "    detected_entities = []\n",
    "\n",
    "    if isinstance(event, list) and event:\n",
    "        event = event[0]\n",
    "\n",
    "    if not isinstance(event, str):\n",
    "        logging.error(f\"Entity detection failed: 'event' is not a string after normalization: {event}\")\n",
    "        return detected_entities\n",
    "\n",
    "    event_lower = event.lower()\n",
    "\n",
    "    stopwords_list = stopwords.words('english')\n",
    "\n",
    "    for entity in entity_list:\n",
    "        entity_name = entity.name\n",
    "        if isinstance(entity_name, list) and entity_name:\n",
    "            entity_name = entity_name[0]\n",
    "\n",
    "        if not isinstance(entity_name, str):\n",
    "            continue\n",
    "\n",
    "        for name_part in entity_name.split():\n",
    "            name_part_lower = name_part.lower()\n",
    "\n",
    "            if name_part_lower in stopwords_list:\n",
    "                continue\n",
    "\n",
    "            if name_part_lower in event_lower:\n",
    "                if entity.name not in detected_entities:\n",
    "                    detected_entities.append(entity.name)\n",
    "                break\n",
    "    return detected_entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "_uqh5qMKFtpP",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_char(n):\n",
    "    \"\"\"Converts a number (1, 2, 3...) to a letter (A, B, C...).\"\"\"\n",
    "    if n < 1:\n",
    "        return '0'\n",
    "    return chr(64 + n)\n",
    "def num_to_roman(n):\n",
    "     import roman\n",
    "     return roman.toRoman(n)\n",
    "\n",
    "class OutlineNode(Sequence):\n",
    "    def pretty(self):\n",
    "        \"\"\"Generates a nicely formatted, hierarchical string representation of the outline.\"\"\"\n",
    "        lines = []\n",
    "        for node in self.depth_first_traverse(include_self=False):\n",
    "             lines.append(node.format_self())\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(d, parent=None):\n",
    "        node = OutlineNode(d['text'], parent, d['scene'], d['entities'], d['id'])\n",
    "        node.children = [OutlineNode.from_dict(child, node) for child in d['children']]\n",
    "        return node\n",
    "\n",
    "    @staticmethod\n",
    "    def num_converter(depth):\n",
    "        if depth == 0:\n",
    "            return lambda num: ''\n",
    "        if depth % 3 == 1:\n",
    "            return str\n",
    "        elif depth % 3 == 2:\n",
    "            return num_to_char\n",
    "        elif depth % 3 == 0:\n",
    "            return num_to_roman\n",
    "\n",
    "    @staticmethod\n",
    "    def indent(depth):\n",
    "        if depth == 0:\n",
    "            return ''\n",
    "        return '\\t' * (depth-1)\n",
    "\n",
    "    def __init__(self, text, parent, scene='', entities=None, id=None):\n",
    "\n",
    "        if isinstance(text, list):\n",
    "            if text:\n",
    "                text = text[0]\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        self.text = text.strip()\n",
    "        self.entities = entities if entities is not None else []\n",
    "        self.scene = scene\n",
    "        self.children = []\n",
    "        self.parent = parent\n",
    "        self.id = str(uuid.uuid4()) if id is None else id\n",
    "        super().__init__()\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'text': self.text,\n",
    "            'scene': self.scene,\n",
    "            'entities': self.entities,\n",
    "            'children': [child.to_dict() for child in self.children],\n",
    "            'id': self.id\n",
    "        }\n",
    "\n",
    "    def format_self(self):\n",
    "        if isinstance(self.text, list) and self.text:\n",
    "            self.text = self.text[0]\n",
    "        if not isinstance(self.text, str):\n",
    "            self.text = \"\"\n",
    "\n",
    "        scene_text = self.scene\n",
    "        if isinstance(scene_text, list) and scene_text:\n",
    "            scene_text = scene_text[0]\n",
    "        if not isinstance(scene_text, str):\n",
    "            scene_text = \"\"\n",
    "\n",
    "        s = self.number() + self.text\n",
    "\n",
    "        if len(scene_text) > 0:\n",
    "            s += ' Scene: ' + scene_text\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        s = f\"{self.number()}: {self.text}\"\n",
    "\n",
    "        for child in self.children:\n",
    "            s += \"\\n\" + str(child)\n",
    "        return s\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.children[index]\n",
    "\n",
    "    def get_node_by_id(self, id):\n",
    "        for node in self.root().depth_first_traverse():\n",
    "            if node.id == id:\n",
    "                return node\n",
    "        return None\n",
    "\n",
    "    def number(self, depth_shift=0, lookforward=0, convert=True):\n",
    "        if self.parent is None:\n",
    "            num = 1\n",
    "        else:\n",
    "            try:\n",
    "                num = self.parent.children.index(self) + 1\n",
    "            except ValueError:\n",
    "                num = 1\n",
    "                for i, child in enumerate(self.parent.children):\n",
    "                    if child.id == self.id:\n",
    "                        num = i + 1\n",
    "                        break\n",
    "\n",
    "        num += lookforward\n",
    "\n",
    "        if convert:\n",
    "            depth = self.depth() + depth_shift\n",
    "            if depth == 0:\n",
    "                return ''\n",
    "            return '\\t' * (depth-1) + OutlineNode.num_converter(depth)(num) + '. '\n",
    "\n",
    "        return num\n",
    "\n",
    "    def depth(self):\n",
    "        if self.parent is None:\n",
    "            return 0\n",
    "        return 1 + self.parent.depth()\n",
    "\n",
    "    def root(self):\n",
    "        if self.parent is None:\n",
    "            return self\n",
    "        return self.parent.root()\n",
    "\n",
    "    def predecessor(self, max_depth=1e8):\n",
    "        nodes = list(self.root().depth_first_traverse(max_depth=max_depth))\n",
    "        idx = nodes.index(self)\n",
    "        return nodes[idx-1] if idx > 0 else None\n",
    "\n",
    "    def successor(self, max_depth=1e8):\n",
    "        nodes = list(self.root().depth_first_traverse(max_depth=max_depth))\n",
    "        idx = nodes.index(self)\n",
    "        return nodes[idx+1] if idx < len(nodes)-1 else None\n",
    "\n",
    "    def ancestors(self, include_self=False):\n",
    "        if self.parent is None:\n",
    "            return [self] if include_self else []\n",
    "        return self.parent.ancestors(include_self=True) + ([self] if include_self else [])\n",
    "\n",
    "    def siblings(self, include_self=False):\n",
    "        if self.parent is None:\n",
    "            return []\n",
    "        return [child for child in self.parent.children if (include_self or child != self)]\n",
    "\n",
    "    def leaves(self):\n",
    "        if len(self.children) == 0:\n",
    "            return [self]\n",
    "        return sum([child.leaves() for child in self.children], [])\n",
    "\n",
    "    def depth_first_traverse(self, include_self=True, max_depth=1e8):\n",
    "        if self.depth() <= max_depth and include_self:\n",
    "            yield self\n",
    "        for child in self.children:\n",
    "            yield from child.depth_first_traverse(max_depth=max_depth)\n",
    "\n",
    "    def breadth_first_traverse(self, include_self=True, max_depth=1e8):\n",
    "        if self.depth() <= max_depth and include_self:\n",
    "            yield self\n",
    "        if self.depth() < max_depth:\n",
    "            queue = [c for c in self.children]\n",
    "            while queue:\n",
    "                n = queue.pop(0)\n",
    "                yield n\n",
    "                if n.depth() < max_depth:\n",
    "                    queue.extend(n.children)\n",
    "\n",
    "    def context(self, context_type):\n",
    "        if context_type == 'full':\n",
    "            selected_nodes = set(list(self.root().depth_first_traverse(include_self=False)))\n",
    "        elif context_type == 'ancestors':\n",
    "            selected_nodes = set(self.ancestors(include_self=False))\n",
    "        elif context_type == 'ancestors-with-siblings':\n",
    "            ancestors = list(self.ancestors(include_self=True))\n",
    "            selected_nodes = set(sum([a.siblings(include_self=True) for a in ancestors], []))\n",
    "        elif context_type == 'ancestors-with-siblings-children':\n",
    "            ancestors = list(self.ancestors(include_self=True))\n",
    "            anc_sibs = sum([a.siblings(include_self=True) for a in ancestors], [])\n",
    "            selected_nodes = set(anc_sibs + sum([node.children for node in anc_sibs], []))\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        prefix = []\n",
    "        suffix = []\n",
    "        in_prefix = True\n",
    "\n",
    "        for node in self.root().depth_first_traverse(include_self=False):\n",
    "            if node == self:\n",
    "                in_prefix = False\n",
    "            elif node in selected_nodes:\n",
    "                (prefix if in_prefix else suffix).append(node)\n",
    "\n",
    "        return (\n",
    "            '\\n\\n'.join([n.format_self() for n in prefix]),\n",
    "            '\\n\\n'.join([n.format_self() for n in suffix])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2Vl1uN8zF3Pz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import string\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "def split_numbered_items(text):\n",
    "\n",
    "    items = re.split(r'\\n?\\s*\\d+\\.\\s*', text)\n",
    "    items = [item.strip() for item in items if item.strip()]\n",
    "    return items\n",
    "\n",
    "\n",
    "def generate_setting(plan, llm_client, setting_prompt, setting_config):\n",
    "    plan.setting = Setting(\n",
    "        llm_client.call_with_retry(\n",
    "            setting_prompt.format(\n",
    "                title=plan.premise.title,\n",
    "                premise=plan.premise.premise\n",
    "            ),\n",
    "            SamplingConfig.from_config(setting_config),\n",
    "            filter=Filter(lambda s: len(s.strip()) > 50),\n",
    "            max_attempts=10\n",
    "        )[0]\n",
    "    )\n",
    "    return plan\n",
    "\n",
    "def generate_entities(plan, llm_client, entity_prompt, entity_config):\n",
    "\n",
    "    normalized = []\n",
    "\n",
    "    if plan.entity_list is not None:\n",
    "        for item in plan.entity_list:\n",
    "            if isinstance(item, Entity):\n",
    "                normalized.append(item)\n",
    "\n",
    "            elif isinstance(item, dict):\n",
    "                normalized.append(Entity(item[\"name\"], item[\"description\"]))\n",
    "\n",
    "            elif isinstance(item, list):\n",
    "                for sub in item:\n",
    "                    if isinstance(sub, Entity):\n",
    "                        normalized.append(sub)\n",
    "                    elif isinstance(sub, dict):\n",
    "                        normalized.append(Entity(sub[\"name\"], sub[\"description\"]))\n",
    "\n",
    "    plan.entity_list = EntityList(normalized)\n",
    "\n",
    "    def postprocess_name(generated, **kwargs):\n",
    "        if not isinstance(generated, (list, tuple)) or not generated:\n",
    "            return [\"\"]\n",
    "\n",
    "        text = str(generated[0]).strip().split(\"\\n\")[0]\n",
    "        text = re.sub(r'^\\d+\\.\\s*', '', text)\n",
    "        return [text.strip()]\n",
    "\n",
    "    def postprocess_entity_description(descriptions, **kwargs):\n",
    "        desc = descriptions[0].split(\"\\n\")[0].strip()\n",
    "        return [desc]\n",
    "\n",
    "    name_config = entity_config['name']\n",
    "    desc_config = entity_config['description']\n",
    "\n",
    "    while len(plan.entity_list) < entity_config['max_entities']:\n",
    "\n",
    "        name = llm_client.call_with_retry(\n",
    "            entity_prompt['name'].format(\n",
    "                title=plan.premise.title,\n",
    "                premise=plan.premise.premise,\n",
    "                setting=plan.setting.setting,\n",
    "                entity_list=\", \".join(e.name for e in plan.entity_list if hasattr(e, 'name') and isinstance(e.name, str)),\n",
    "            ),\n",
    "            SamplingConfig.from_config(name_config),\n",
    "            postprocessor=postprocess_name,\n",
    "            filter=Filter(lambda s: len(s.strip()) > 1),\n",
    "            max_attempts=10\n",
    "        )[0]\n",
    "\n",
    "        if isinstance(name, list):\n",
    "           if name:\n",
    "             name = name[0]\n",
    "           else:\n",
    "             name = \"\"\n",
    "        name = str(name).strip()\n",
    "\n",
    "\n",
    "        if name in [e.name for e in plan.entity_list]:\n",
    "            break\n",
    "\n",
    "        desc = llm_client.call_with_retry(\n",
    "            entity_prompt['description'].format(\n",
    "                title=plan.premise.title,\n",
    "                premise=plan.premise.premise,\n",
    "                setting=plan.setting.setting,\n",
    "                entity_name=name\n",
    "            ),\n",
    "            SamplingConfig.from_config(desc_config),\n",
    "            postprocessor=postprocess_entity_description,\n",
    "            filter=Filter(lambda s: len(s.strip()) > 10),\n",
    "            max_attempts=10\n",
    "        )[0]\n",
    "\n",
    "        if isinstance(desc, list):\n",
    "           if desc:\n",
    "             desc = desc[0]\n",
    "           else:\n",
    "             desc = \"\"\n",
    "        desc = str(desc).strip()\n",
    "\n",
    "\n",
    "        plan.entity_list.entities.append(Entity(name, desc))\n",
    "\n",
    "    return plan\n",
    "\n",
    "def generate_outline(plan, llm_client, outline_prompt, outline_config):\n",
    "    plan.outline = OutlineNode('', None)\n",
    "    max_nodes = 50\n",
    "    while True:\n",
    "        if len(list(plan.outline.depth_first_traverse())) > max_nodes:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_to_expand = select_node_to_expand(plan.outline, outline_config)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        generate_node_subevents(node_to_expand, llm_client, outline_prompt, outline_config, plan, max_attempts=10)\n",
    "\n",
    "    return plan\n",
    "\n",
    "def generate_node_subevents(node, llm_client, outline_prompt, outline_config, plan, max_attempts=1):\n",
    "    context_prefix = \"\"\n",
    "    context_suffix = \"\"\n",
    "    filter = Filter(lambda x: True)\n",
    "\n",
    "    def event_postprocessor(events, **kwargs):\n",
    "        responses = []\n",
    "        for event in events:\n",
    "            event = event.strip()\n",
    "            event = re.sub(r'^\\[[^\\]]*\\]\\s*', '', event)\n",
    "            event = event.split('\\n')[0]\n",
    "            event = event.split('Scene:')[0]\n",
    "            event = event.split('Characters:')[0]\n",
    "            event = event.strip()\n",
    "\n",
    "            if not event:\n",
    "                event = \"Something happens.\"\n",
    "\n",
    "            if event[-1] not in \".?!\":\n",
    "                event += \".\"\n",
    "\n",
    "            responses.append(event)\n",
    "        return responses\n",
    "\n",
    "    if node.depth() == 0:\n",
    "        event_config = outline_config['event_depth_0']\n",
    "        event_prompt = outline_prompt['event_depth_0']\n",
    "    else:\n",
    "        event_config = outline_config['event']\n",
    "        event_prompt = outline_prompt['event']\n",
    "\n",
    "    for _ in range(outline_config['preferred_max_children']):\n",
    "        new_child = OutlineNode('', node)\n",
    "\n",
    "        event = llm_client.call_with_retry(\n",
    "            event_prompt.format(\n",
    "                title=plan.premise.title,\n",
    "                premise=plan.premise.premise,\n",
    "                setting=plan.setting.setting,\n",
    "                entities=str(plan.entity_list),\n",
    "                formatted_current_number=new_child.number().rstrip(),\n",
    "                stripped_current_number=new_child.number().strip(),\n",
    "                context_prefix=context_prefix,\n",
    "                context_suffix=context_suffix,\n",
    "                predecessor_info=\"\",\n",
    "                successor_info=\"\",\n",
    "                preferred_max_children=outline_config['preferred_max_children']\n",
    "            ),\n",
    "            SamplingConfig.from_config(event_config),\n",
    "            postprocessor=partial(\n",
    "                event_postprocessor,\n",
    "                has_next_indicator=\"\\n\" + new_child.number(lookforward=1).strip(),\n",
    "                current_number=new_child.number().strip()\n",
    "            ),\n",
    "            filter=filter,\n",
    "            max_attempts=max_attempts\n",
    "        )\n",
    "\n",
    "        logging.warning(f\"Raw LLM event: {event}\")\n",
    "\n",
    "        new_child.text = event[0]\n",
    "        node.children.append(new_child)\n",
    "\n",
    "        context_prefix, context_suffix = new_child.context(outline_config['context'])\n",
    "\n",
    "        if len(node.children) >= outline_config['max_children']:\n",
    "            break\n",
    "\n",
    "        filter = Filter(lambda x: True)\n",
    "\n",
    "        generate_node_scene(\n",
    "            new_child, llm_client,\n",
    "            outline_prompt['scene'], outline_config['scene'], plan\n",
    "        )\n",
    "\n",
    "        generate_node_entities(\n",
    "            new_child, llm_client,\n",
    "            outline_prompt['entity_depth_0'] if node.depth() == 0 else outline_prompt['entity'],\n",
    "            outline_config['entity_depth_0'] if node.depth() == 0 else outline_config['entity'],\n",
    "            plan\n",
    "        )\n",
    "\n",
    "def generate_node_scene(node, llm_client, scene_prompt, scene_config, plan):\n",
    "    def scene_postprocessor(scenes, **kwargs):\n",
    "        clean = []\n",
    "        for sc in scenes:\n",
    "            sc = sc.split('\\n')[0].split('Characters:')[0].split('Scene:')[-1].strip()\n",
    "            clean.append(sc)\n",
    "        return clean\n",
    "\n",
    "    context_prefix, context_suffix = node.context(scene_config['context'])\n",
    "\n",
    "    node.scene = llm_client.call_with_retry(\n",
    "        scene_prompt.format(\n",
    "            title=plan.premise.title,\n",
    "            premise=plan.premise.premise,\n",
    "            setting=plan.setting.setting,\n",
    "            entities=str(plan.entity_list),\n",
    "            formatted_current_number=node.number().rstrip(),\n",
    "            stripped_current_number=node.number().strip(),\n",
    "            current_event=node.text,\n",
    "            context_prefix=context_prefix,\n",
    "            context_suffix=context_suffix\n",
    "        ),\n",
    "        SamplingConfig.from_config(scene_config),\n",
    "        postprocessor=scene_postprocessor,\n",
    "        filter=Filter(lambda s: len(s.strip()) > 0),\n",
    "    )[0]\n",
    "\n",
    "def generate_node_entities(node, llm_client, entity_prompt, entity_config, plan):\n",
    "    detected = detect_entities(node.text, plan.entity_list)\n",
    "    if detected:\n",
    "       node.entities = detected\n",
    "       return\n",
    "\n",
    "    def entity_postprocessor(predicted_lists, entity_list, already_detected, **kwargs):\n",
    "        out = []\n",
    "        for ents in predicted_lists:\n",
    "            ents = ents.split('\\n')[0].strip().rstrip('.')\n",
    "            ents = [e.strip() for e in ents.split(',')]\n",
    "            ents = [e for e in ents if e in [x.name for x in entity_list]]\n",
    "            ents = list(dict.fromkeys(ents))\n",
    "            ents = [e for e in ents if e not in already_detected]\n",
    "            out.append(already_detected + ents)\n",
    "        return out\n",
    "\n",
    "    detected = detect_entities(node.text[0], plan.entity_list)\n",
    "    context_prefix, context_suffix = node.context(entity_config['context'])\n",
    "\n",
    "    try:\n",
    "        node.entities = llm_client.call_with_retry(\n",
    "            entity_prompt.format(\n",
    "                title=plan.premise.title,\n",
    "                premise=plan.premise.premise,\n",
    "                setting=plan.setting.setting,\n",
    "                entities=str(plan.entity_list),\n",
    "                formatted_current_number=node.number().rstrip(),\n",
    "                stripped_current_number=node.number().strip(),\n",
    "                current_event=node.text,\n",
    "                current_scene=node.scene,\n",
    "                context_prefix=context_prefix,\n",
    "                context_suffix=context_suffix,\n",
    "                detected_entities=\", \".join(detected)\n",
    "            ),\n",
    "            SamplingConfig.from_config(entity_config),\n",
    "            postprocessor=partial(entity_postprocessor,\n",
    "                                  entity_list=plan.entity_list,\n",
    "                                  already_detected=detected),\n",
    "            filter=Filter(lambda l: len(l) > 0),\n",
    "            max_attempts=20\n",
    "        )[0]\n",
    "\n",
    "    except Exception:\n",
    "        node.entities = detected\n",
    "\n",
    "def select_node_to_expand(outline, outline_config):\n",
    "    if outline_config['expansion_policy'] == 'breadth-first':\n",
    "        for node in outline.breadth_first_traverse(max_depth=outline_config['max_depth'] - 1):\n",
    "            if len(node.children) == 0:\n",
    "                return node\n",
    "        raise StopIteration\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9ROmt0nU18H",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 15:15:29 INFO     Generating setting...\n",
      "2025-12-05 15:16:02 INFO     Generated setting: \"As the seed sprouts, Marigold's garden becomes a bustling, rainbow-colored haven filled with chirping birds, buzzing bees, and butterflies flitting about. The sun shines bright, casting a warm, golden glow on the blooming flowers and the growing plant.\n",
      "\n",
      "'Sprout, little seed!' Marigold whispers, as she tenderly waters her new friend each day. She feels the cool water drip from the watering can, down onto the soil, and watches as the green shoot grows taller and stronger.\n",
      "\n",
      "'Look at you grow!' Marigold exclaims, as the plant stretches its leaves towards the sky. The leaves start to unfurl, revealing tiny, delicate flowers that sparkle in the sunlight.\n",
      "\n",
      "'You're beautiful!' Marigold says, as she admires her creation. She feels a sense of pride and happiness, knowing that she's helped something grow from a tiny seed into a beautiful plant.\n",
      "\n",
      "One day, Marigold's plant blooms, filling her garden with a sweet, fragrant scent. Marigold's heart swells with joy, as she realizes that she'\n",
      "2025-12-05 15:16:02 INFO     Generating entities...\n",
      "2025-12-05 15:16:58 INFO     Generated entities: 1. \"Petunia\" (The name of the plant that Marigold n: \"Petunia\" is a vibrant, growing plant that Marigold nurtures from a tiny seed in her sun-kissed garden. What makes Petunia special is not just her beauty, but the bond she shares with Marigold. Through Petunia's\n",
      "\n",
      "2. Blossom (The name of the plant that Marigold nurtures: \"Blossom is a vibrant, growing plant that Marigold nurtures with love and care. With tiny, delicate flowers that sparkle in the sunlight, Blossom is a symbol of life's growth and the magic of nature, teaching Marigold about patience, responsibility\n",
      "\n",
      "3. \"Sunny\" (The name of the new plant that Marigold n: \"Sunny\" is a small, vibrant plant that Marigold nurtures from a seed in her garden. What makes Sunny special is not just its beautiful, fragrant blooms, but the bond of love and care that Marigold cultivates with it, teaching both of\n",
      "\n",
      "4. \"Sunny Blossom\": \"Sunny Blossom, a cheerful and nurturing 8-year-old with a contagious smile, is the heart of Gardenia Village. She has bright, sparkling green eyes that twinkle with curiosity and kindness, and her golden curls are always adorn\n",
      "2025-12-05 15:16:58 INFO     Generating outline...\n",
      "2025-12-05 15:17:14 WARNING  Raw LLM event: (['5. \"Grandma Rose\": \"Grandma Rose, a wise and gentle woman with a heart as warm as the sun, is Marigold\\'s beloved grandmother. She is a master gardener, sharing her vast knowledge of plants and the magic of nature with Marigold. Her kindness and patience inspire Marigold to become a responsible and caring gardener like her.\".'], None)\n",
      "2025-12-05 15:17:45 WARNING  Raw LLM event: (['4. Grandma Elara: \"Grandma Elara\" is a wise, loving, and nurturing figure in Marigold\\'s life. She has a green thumb and is a well-respected gardener in Gardenia Village. Her garden, which is filled with a variety of colorful and exotic plants, serves as an inspiration for Marigold.'], None)\n",
      "2025-12-05 15:18:00 INFO     Generated outline with 3 nodes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL RESULT ---\n",
      "Title: ['\"Seed Sprouts Surprise: A Sunlit Tale of Growth and Green Thumbs\"']\n",
      "\n",
      "Premise: [\"In a vibrant, sun-kissed Gardenia Village, where every house has a blooming garden, lives a curious and kind-hearted child named Marigold. One sunny morning, Marigold finds a small, sleepy seed in her grandmother's garden. With a twinkle in her eyes, she decides to nurture this tiny promise of a plant. Marigold carefully places the seed in a pot filled with nutrient-rich soil and waters it gently. As days pass, she watches and waits, watering and talking to her new friend, hoping and praying for it to grow. One magical day, a green shoot pokes through the soil, reaching up towards the sky. Marigold cheers, realizing that she's not just growing a plant, but also learning the secrets of life - patience, love, and the magic of nature. This exciting adventure of growing a plant from a seed teaches Marigold about responsibility\"]\n",
      "\n",
      "Setting:\n",
      "\"As the seed sprouts, Marigold's garden becomes a bustling, rainbow-colored haven filled with chirping birds, buzzing bees, and butterflies flitting about. The sun shines bright, casting a warm, golden glow on the blooming flowers and the growing plant.\n",
      "\n",
      "'Sprout, little seed!' Marigold whispers, as she tenderly waters her new friend each day. She feels the cool water drip from the watering can, down onto the soil, and watches as the green shoot grows taller and stronger.\n",
      "\n",
      "'Look at you grow!' Marigold exclaims, as the plant stretches its leaves towards the sky. The leaves start to unfurl, revealing tiny, delicate flowers that sparkle in the sunlight.\n",
      "\n",
      "'You're beautiful!' Marigold says, as she admires her creation. She feels a sense of pride and happiness, knowing that she's helped something grow from a tiny seed into a beautiful plant.\n",
      "\n",
      "One day, Marigold's plant blooms, filling her garden with a sweet, fragrant scent. Marigold's heart swells with joy, as she realizes that she'\n",
      "\n",
      "Characters and Entities:\n",
      "1. \"Petunia\" (The name of the plant that Marigold n: \"Petunia\" is a vibrant, growing plant that Marigold nurtures from a tiny seed in her sun-kissed garden. What makes Petunia special is not just her beauty, but the bond she shares with Marigold. Through Petunia's\n",
      "\n",
      "2. Blossom (The name of the plant that Marigold nurtures: \"Blossom is a vibrant, growing plant that Marigold nurtures with love and care. With tiny, delicate flowers that sparkle in the sunlight, Blossom is a symbol of life's growth and the magic of nature, teaching Marigold about patience, responsibility\n",
      "\n",
      "3. \"Sunny\" (The name of the new plant that Marigold n: \"Sunny\" is a small, vibrant plant that Marigold nurtures from a seed in her garden. What makes Sunny special is not just its beautiful, fragrant blooms, but the bond of love and care that Marigold cultivates with it, teaching both of\n",
      "\n",
      "4. \"Sunny Blossom\": \"Sunny Blossom, a cheerful and nurturing 8-year-old with a contagious smile, is the heart of Gardenia Village. She has bright, sparkling green eyes that twinkle with curiosity and kindness, and her golden curls are always adorn\n",
      "\n",
      "Outline:\n",
      ": \n",
      "1. : 5. \"Grandma Rose\": \"Grandma Rose, a wise and gentle woman with a heart as warm as the sun, is Marigold's beloved grandmother. She is a master gardener, sharing her vast knowledge of plants and the magic of nature with Marigold. Her kindness and patience inspire Marigold to become a responsible and caring gardener like her.\".\n",
      "2. : ['4. Grandma Elara: \"Grandma Elara\" is a wise, loving, and nurturing figure in Marigold\\'s life. She has a green thumb and is a well-respected gardener in Gardenia Village. Her garden, which is filled with a variety of colorful and exotic plants, serves as an inspiration for Marigold.']\n",
      "\n",
      "Plan object saved to: output/plan.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    prompts_dict = json.loads(prompts_json_content)\n",
    "    prompts = load_prompts_from_dict(prompts_dict)\n",
    "    premise = Premise.load(config['premise_path'])\n",
    "\n",
    "    plan = Plan(premise)\n",
    "\n",
    "    plan_config = config['model']['plan']\n",
    "    plan_prompts = prompts['plan']\n",
    "\n",
    "    logging.info(\"Generating setting...\")\n",
    "    plan = generate_setting(\n",
    "        plan,\n",
    "        llm_client,\n",
    "        plan_prompts['setting'],\n",
    "        plan_config['setting']\n",
    "    )\n",
    "    logging.info(f\"Generated setting: {plan.setting.setting}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    logging.info(\"Generating entities...\")\n",
    "    plan = generate_entities(\n",
    "        plan,\n",
    "        llm_client,\n",
    "        plan_prompts['entity'],\n",
    "        plan_config['entity']\n",
    "    )\n",
    "    logging.info(f\"Generated entities: {plan.entity_list}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    logging.info(\"Generating outline...\")\n",
    "    plan = generate_outline(\n",
    "        plan,\n",
    "        llm_client,\n",
    "        plan_prompts['outline'],\n",
    "        plan_config['outline']\n",
    "    )\n",
    "    logging.info(f\"Generated outline with {len(list(plan.outline.depth_first_traverse()))} nodes.\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    output_path = config['output_path']\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    plan.save(output_path)\n",
    "\n",
    "    print(\"\\n--- FINAL RESULT ---\")\n",
    "    print(plan)\n",
    "    print(f\"\\nPlan object saved to: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during execution. Please check your model configuration and ensure your LLM server is running and accessible. Error: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlAkZw1p-tGH",
   "metadata": {
    "id": "mlAkZw1p-tGH"
   },
   "source": [
    "## Generate the final story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "_2DpU5GswCyr",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Story:\n",
    "    def __init__(self):\n",
    "        self.passages = []\n",
    "\n",
    "    def add_passage(self, passage_dict):\n",
    "        self.passages.append(passage_dict)\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"story\": self.passages\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def __str__(self):\n",
    "        all_text = \"\\n\\n\".join([p[\"text\"] for p in self.passages])\n",
    "        return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "US6EdCJov3iD",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story Prompts loaded and templates created.\n"
     ]
    }
   ],
   "source": [
    "prompts_story_json_content = \"\"\"\n",
    "{\n",
    "  \"story\": {\n",
    "    \"write\": {\n",
    "      \"instruction\": \"Write the next passage of the children's story. Use short sentences, simple words, and fun descriptions. Keep the tone cheerful and engaging. Story so far://{story_so_far}//Outline Event://{outline_event}//Scene://{scene}//Characters://{entities}//Write a clear, kid-friendly passage:\",\n",
    "      \"response_prefix\": \"\"\n",
    "    },\n",
    "    \"score\": {\n",
    "      \"instruction\": \"Rate the quality of this passage for a children's story. Focus on clarity, fun, simplicity, and imagination. Return a number from 1 to 10. Passage://{passage}//Score:\",\n",
    "      \"response_prefix\": \"\"\n",
    "    },\n",
    "    \"summarize\": {\n",
    "      \"instruction\": \"Summarize this passage into 12 sentences for kids. Use simple language and keep it fun. Passage://{passage}//Summary:\",\n",
    "      \"response_prefix\": \"\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "story_prompts_dict = json.loads(prompts_story_json_content)\n",
    "story_prompts = load_prompts_from_dict(story_prompts_dict)\n",
    "print(\"Story Prompts loaded and templates created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "rBawvpnqwN64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryWriter:\n",
    "    def __init__(self, llm_client, prompts, config):\n",
    "        self.llm = llm_client\n",
    "        self.prompts = prompts\n",
    "        self.config_write = SamplingConfig.from_config(config[\"write\"])\n",
    "        self.config_score = SamplingConfig.from_config(config[\"score\"])\n",
    "        self.config_summarize = SamplingConfig.from_config(config[\"summarize\"])\n",
    "\n",
    "    def generate_passage(self, story_so_far, outline_event, scene, entities):\n",
    "        prompt_builder = self.prompts[\"write\"].format(\n",
    "            story_so_far=story_so_far,\n",
    "            outline_event=outline_event,\n",
    "            scene=scene,\n",
    "            entities=\", \".join(entities)\n",
    "        )\n",
    "        result = self.llm.call_with_retry(\n",
    "            prompt_builder,\n",
    "            self.config_write,\n",
    "            max_attempts=3\n",
    "        )[0]\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        return result\n",
    "\n",
    "\n",
    "    def score_passage(self, passage):\n",
    "        prompt_builder = self.prompts[\"score\"].format(\n",
    "            passage=passage\n",
    "        )\n",
    "        result = self.llm.call_with_retry(\n",
    "            prompt_builder,\n",
    "            self.config_score,\n",
    "            max_attempts=2\n",
    "        )[0]\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        return result.strip()\n",
    "\n",
    "\n",
    "    def summarize_passage(self, passage):\n",
    "        prompt_builder = self.prompts[\"summarize\"].format(\n",
    "            passage=passage\n",
    "        )\n",
    "        result = self.llm.call_with_retry(\n",
    "            prompt_builder,\n",
    "            self.config_summarize,\n",
    "            max_attempts=2\n",
    "        )[0]\n",
    "\n",
    "        if isinstance(result, list):\n",
    "            result = result[0]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "Wa7p5l9evvd_",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "story_config_yaml = \"\"\"\n",
    "model:\n",
    "  engine: \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "  host: \"http://localhost\"\n",
    "  port: 8000\n",
    "  server_type: vllm\n",
    "  tensor_parallel_size: 1\n",
    "\n",
    "  story:\n",
    "    write:\n",
    "      max_tokens: 350\n",
    "      temperature: 0.7\n",
    "      top_p: 0.9\n",
    "      prompt_format: openai-chat\n",
    "    score:\n",
    "      max_tokens: 20\n",
    "      temperature: 0.1\n",
    "      top_p: 0.5\n",
    "      prompt_format: openai-chat\n",
    "    summarize:\n",
    "      max_tokens: 80\n",
    "      temperature: 0.3\n",
    "      top_p: 0.9\n",
    "      prompt_format: openai-chat\n",
    "\n",
    "output_path: \"outputs/story.json\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "story_config = Config(yaml.safe_load(story_config_yaml), None)\n",
    "print(\"Story configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "DocYIm3Euaoj",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 15:18:00 INFO     Loading plan...\n",
      "2025-12-05 15:18:00 INFO     Configuration and prompts are already loaded globally.\n",
      "2025-12-05 15:18:00 INFO     Generating final story...\n",
      "2025-12-05 15:18:00 INFO     Beginning story generation from outline...\n",
      "2025-12-05 15:18:00 INFO     Generating passage for node : \n",
      "2025-12-05 15:18:58 INFO     Generating passage for node 1. : 5. \"Grandma Rose\": \"Grandma Rose, a wise and gentle woman with a heart as warm as the sun, is Marigold's beloved grandmother. She is a master gardener, sharing her vast knowledge of plants and the magic of nature with Marigold. Her kindness and patience inspire Marigold to become a responsible and caring gardener like her.\".\n",
      "2025-12-05 15:19:59 INFO     Generating passage for node 2. : 4. Grandma Elara: \"Grandma Elara\" is a wise, loving, and nurturing figure in Marigold's life. She has a green thumb and is a well-respected gardener in Gardenia Village. Her garden, which is filled with a variety of colorful and exotic plants, serves as an inspiration for Marigold.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL STORY GENERATED SUCCESSFULLY ---\n",
      "Saved to outputs/story.json\n",
      "\n",
      "--- FINAL STORY ---\n",
      "In a bright, sunny meadow, where daisies danced and butterflies fluttered, lived a tiny, sprightly squirrel named Squeaky. Squeaky loved to race up and down the tallest trees, chasing the wind and collecting acorns.\n",
      "\n",
      "One sunny afternoon, Squeaky found a shiny, golden acorn hidden beneath a patch of clover. It was bigger than any acorn he had ever seen! Squeaky picked it up and held it close, feeling its warm, golden glow.\n",
      "\n",
      "\"Oh, what a treasure!\" Squeaky exclaimed. \"I wonder what this could be!\"\n",
      "\n",
      "Just then, a friendly, talking owl named Oliver swooped down from a nearby tree. \"Hello, Squeaky!\" he greeted. \"What have you got there?\"\n",
      "\n",
      "\"I found this golden acorn!\" Squeaky replied, showing it to Oliver. \"I've never seen anything like it before!\"\n",
      "\n",
      "Oliver's eyes widened with curiosity. \"A golden acorn? That's quite unusual! It's said that golden acorns hold magic within them. Perhaps it's a sign of a magical adventure!\"\n",
      "\n",
      "Squeaky's eyes sparkled with excitement. \"A magical adventure? Count me in, Oliver! Let's find out what this golden acorn can do!\"\n",
      "\n",
      "And so, Squeaky and Oliver embarked on an exciting journey, filled with enchanting sights, puzzling riddles, and wonderful friends they met along the way. Little did they know, their adventure was just beginning!\n",
      "\n",
      "\"One day, while tending to her garden, Marigold noticed a new flower bud sprouting. She named it Sunny, and she couldn't wait to see it bloom.\n",
      "\n",
      "As the days passed, Sunny grew bigger and brighter. Marigold watered it carefully and spoke kind words to it every day. Soon, Sunny bloomed into a beautiful, yellow flower, just like Petunia and Blossom.\n",
      "\n",
      "\"Look at Sunny!\" Marigold exclaimed. \"It's just as beautiful as Petunia and Blossom!\"\n",
      "\n",
      "Just then, a friendly buzzing sound caught her attention. It was Mr. Bee, buzzing around Sunny, collecting pollen.\n",
      "\n",
      "\"Hello, Marigold!\" Mr. Bee greeted. \"You've done a wonderful job taking care of Sunny. It's just as vibrant and healthy as Petunia and Blossom.\"\n",
      "\n",
      "Marigold smiled, feeling proud. \"Thank you, Mr. Bee! I've been learning from Grandma Rose. She's taught me so much about gardening.\"\n",
      "\n",
      "Mr. Bee nodded. \"Grandma Rose is a wonderful gardener. Her love and care for plants are truly magical.\"\n",
      "\n",
      "Marigold agreed. \"I want to be just like Grandma Rose when I grow up.\"\n",
      "\n",
      "And so, Marigold continued to care for her garden, learning from Grandma Rose and Mr. Bee. Each day, she felt more and more like a real gardener, and she knew that with time and patience, her garden would continue to grow and blo\n",
      "\n",
      "\"One day, Marigold decided to visit Grandma Elara in her beautiful garden. She was excited to learn more about gardening from her. As she walked through the garden, she admired the colorful and exotic plants that Grandma Elara had grown.\n",
      "\n",
      "\"Hello, Marigold!\" Grandma Elara greeted, smiling warmly. \"I've been looking forward to seeing you! I've heard about your garden and how well you've been taking care of it.\"\n",
      "\n",
      "Marigold blushed, feeling proud. \"Thank you, Grandma Elara. I've been learning from you and Mr. Bee.\"\n",
      "\n",
      "Grandma Elara nodded. \"I'm glad to hear that. You're doing a wonderful job, my dear. Now, let me show you something special.\"\n",
      "\n",
      "She led Marigold to a hidden corner of the garden, where a rare, blue flower was blooming. It was even more beautiful than Petunia, Blossom, and Sunny combined!\n",
      "\n",
      "\"This is my pride and joy,\" Grandma Elara said, her eyes shining with pride. \"It's called the Blue Mystic. It's a very rare and precious flower. I've been caring for it for years, and it's finally bloomed.\"\n",
      "\n",
      "Marigold gasped in awe. \"Wow, Grandma Elara! It's stunning! I can't wait to show it to Mr. Bee and Petunia, Blossom, and Sunny!\"\n",
      "\n",
      "Grandma Elara smiled. \"I'm glad you like it. Remember,\n"
     ]
    }
   ],
   "source": [
    "def generate_story(plan, llm_client, prompts, config):\n",
    "    story_writer = StoryWriter(\n",
    "        llm_client=llm_client,\n",
    "        prompts=prompts[\"story\"],\n",
    "        config=config[\"model\"][\"story\"]\n",
    "    )\n",
    "    story = Story()\n",
    "    logging.info(\"Beginning story generation from outline...\")\n",
    "    outline_nodes = list(plan.outline.depth_first_traverse())\n",
    "\n",
    "    story_text_so_far = \"\"\n",
    "    for node in outline_nodes:\n",
    "        logging.info(f\"Generating passage for node {node.number()}: {node.text}\")\n",
    "        passage = story_writer.generate_passage(\n",
    "            story_so_far=story_text_so_far,\n",
    "            outline_event=node.text,\n",
    "            scene=node.scene,\n",
    "            entities=node.entities\n",
    "        )\n",
    "        summary = story_writer.summarize_passage(passage)\n",
    "        score = story_writer.score_passage(passage)\n",
    "\n",
    "        story.add_passage({\n",
    "            \"event_number\": node.number(),\n",
    "            \"text\": passage,\n",
    "            \"summary\": summary,\n",
    "            \"score\": score,\n",
    "            \"entities\": node.entities,\n",
    "            \"scene\": node.scene\n",
    "        })\n",
    "        story_text_so_far += \"\\n\" + passage\n",
    "    return story\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logging.info(\"Loading plan...\")\n",
    "        plan_path = \"output/plan.json\"\n",
    "\n",
    "        plan = Plan.load(plan_path)\n",
    "\n",
    "        logging.info(\"Configuration and prompts are already loaded globally.\")\n",
    "\n",
    "        logging.info(\"Generating final story...\")\n",
    "        story = generate_story(\n",
    "            plan,\n",
    "            llm_client,\n",
    "            story_prompts,\n",
    "            story_config[\"model\"][\"story\"]\n",
    "        )\n",
    "\n",
    "        output_path = story_config[\"output_path\"]\n",
    "        story.save(output_path)\n",
    "\n",
    "        print(\"\\n--- FINAL STORY GENERATED SUCCESSFULLY ---\")\n",
    "        print(f\"Saved to {output_path}\")\n",
    "        print(\"\\n--- FINAL STORY ---\")\n",
    "        print(story)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Story Generation failed: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B3n-qvyiV7pR",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2b53b2-6bfd-4306-afbb-ed784735ca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
