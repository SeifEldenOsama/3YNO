# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pE-B7eW7jpR8X9kKKEAwdoC7UvrhmGmW
"""

import pandas as pd
import numpy as np
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer
from config import CSV_PATH, MAX_INPUT_LENGTH, MAX_TARGET_LENGTH, SEED
import random
import torch

def load_dataset(tokenizer):

    df = pd.read_csv(CSV_PATH)
    df = df.rename(columns={'lesson_text': 'article', 'summary': 'summary'})
    df = df[['article', 'summary']].dropna()

    # Remove corrupted chars
    mask = df["article"].str.contains("�") | df["summary"].str.contains("�")
    df = df[~mask].reset_index(drop=True)

    dataset = Dataset.from_pandas(df)

    # split
    train_val_test_split = dataset.train_test_split(test_size=200, seed=SEED)
    train_val_split = train_val_test_split["train"].train_test_split(test_size=0.1, seed=SEED)

    dataset = DatasetDict({
        "train": train_val_split["train"],
        "validation": train_val_split["test"],
        "test": train_val_test_split["test"],
    })

    def preprocess(examples):
        model_inputs = tokenizer(
            examples["article"],
            max_length=MAX_INPUT_LENGTH,
            truncation=True,
            padding="max_length",
        )

        with tokenizer.as_target_tokenizer():
            labels = tokenizer(
                examples["summary"],
                max_length=MAX_TARGET_LENGTH,
                truncation=True,
                padding="max_length",
            )

        labels["input_ids"] = [
            [(token if token != tokenizer.pad_token_id else -100)
             for token in seq]
            for seq in labels["input_ids"]
        ]

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized = dataset.map(
        preprocess,
        batched=True,
        remove_columns=dataset["train"].column_names,
    )

    # LED global attention mask
    def add_global_attention(batch):
        batch["global_attention_mask"] = [
            [1] + [0]*(len(input_ids)-1) for input_ids in batch["input_ids"]
        ]
        return batch

    tokenized = tokenized.map(add_global_attention, batched=True)
    return tokenized, dataset